# Construcci칩n de Pipeline con YFinance, Airbyte, Clickhouse, DBT y Airflow

Este proyecto tiene como objetivo integrar y validar m칰ltiples fuentes de informaci칩n sobre bancos que cotizan en la bolsa de valores de los Estados Unidos. Se utiliza un pipeline de datos que incluye la extracci칩n de la informaci칩n con Yfinance, un entorno de landing zone en PostgreSQL, un almac칠n OLAP en ClickHouse, herramientas de integraci칩n como Airbyte, validaci칩n y transformaci칩n de datos con DBT, y orquestaci칩n del pipeline con Airflow.

Este es el diagrama de la arquitectura del ETL con las tecnolog칤as utilizadas:

![Arquitectura del ETL](https://github.com/user-attachments/assets/f2a020cf-3519-4836-94e3-f50b94801dad)

Y este est치 el diagrama del ETL al final de la implementaci칩n desde la vista de grafos del Dag Airflow:

Este es el DAG que ejecuta la extracci칩n y la carga de la data desde el source hasta el stage:
![ELT Dag](https://github.com/user-attachments/assets/f7d53a23-bf82-494f-a6cb-5f03ac15934d)

Este es el DAG que ejecuta las transformaciones con DBT cuando hubo alg칰n cambio en el datawarehouse:
![DBT jobs](https://github.com/user-attachments/assets/e5f1e019-eb41-4c33-8864-d2612c030cd4)


## Tecnolog칤as usadas:
- **PostgreSQL**: Para almacenar datos crudos extra칤dos desde el API.
- **ClickHouse**: Como almac칠n centralizado de datos curados.
- **Airbyte**: Para la integraci칩n de datos entre PostgreSQL y ClickHouse.
- **DBT**: Para validar y transformar los datos.
- **Airflow**: Para orquestar y automatizar el pipeline de datos.
- **yfinance**: Para la extracci칩n de informaci칩n sobre los bancos.

## Requerimientos previos:

Antes de comenzar con esta integraci칩n, aseg칰rate de tener las siguientes instrucciones listas:

1. **Git**: Si no lo tiene instalado, desc치rgalo e inst치lalo desde [Git][[https://docs.docker.com/get-docker/](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)] siguiendo la documentaci칩n oficial para tu Sistema Operativo.

2. **Docker y Docker Compose (Docker Desktop)**: Si no lo tiene instalado, desc치rgalo e inst치lalo desde [Docker][https://docs.docker.com/get-docker/] siguiendo la documentaci칩n oficial para tu Sistema Operativo.

## 1. Configurar el ambiente para el proyecto

Descargue el proyecto en su m치quina local siguiendo estos pasos:

1. **Clona este repositorio**:
   ```bash
   git clone https://github.com/gabrielcepedag/sib.git
   ```
2. **Clona el repositorio de AirByte**:
   ```bash
   git clone https://github.com/airbytehq/airbyte.git
   ```
3. **Crea un archivo .env en la ra칤z del proyecto con estas variables un ejemplo del valor**:
   ```bash
   cd sib/
   ```

   ```bash
   #POSTGRESQL
    DB_HOST=localhost
    POSTGRES_USER=sib_user
    POSTGRES_PASSWORD=sib_user
    POSTGRES_DB=landing_zone
    POSTGRES_PORT=5432
    
    #CLICKHOUSE
    CLICKHOUSE_HTTP_PORT=8123
    CLICKHOUSE_TCP_PORT=9000
    
    #AIRFLOW
    AIRFLOW_DB=airflow
    AIRFLOW_WEB_PORT=8080
    #id - u en el host
    AIRFLOW_UID=501
   ```

### 2. Configurar la red para docker
  
  Dado que el servicio de Airbyte se encuentra en otro proyecto, es necesario crear una red externa de forma que
  desde el contenedor en donde se ejecuta Airflow, se tenga comunicaci칩n con el contenedor que maneja las conexiones
  de Airbyte. Para ello siga los siguientes pasos: 

  1. Crear la red en docker
     
   ```bash
   docker network create airbyte_airflow
   ```

## 3. Ejecutar los contenedores

  En esta paso usted puede proceder con la ejecuci칩n de los contenedores de ambos proyectos para ello, siga las siguientes instrucciones:
     
  1. Correr las dem치s herramientas:

   ```bash
    cd sib/
   ```
   ```bash
    docker-compose up -d
   ```

   2. Este comando descarga el archivo docker-compose.yaml y corre Airbyte. Por lo tanto, primero lo ejecutamos para descargar el archivo:

   ```bash
    ./airbyte/run-ab-platform.sh
   ```

   3. Luego, agregamos la nueva red en el docker-compose.yml del proyecto de Airbyte como se muestra en adelante:
   
   ```bash
   networks:
    airbyte_airflow:
    external: true
   ```

  3. Agregar la red en el servicio de airbyte-proxy:
   
   ```bash
    airbyte-proxy:
      networks:
      - airbyte_airflow
   ```
   
## 4. Configurar conector de Airbyte desde la UI

Inicia en la UI de Airbyte accediendo a http://localhost:8000/ en tu navegador. El usuario y contrase침a por defecto es airbyte y password. Luego:

  1. **Crea una fuente (source)**:

   - Ve al apartado de Sources y click en `+ New source`.
   - Busca el conector para `Postgres`.
   - Ajusta los campos seg칰n tus variables de entorno. Una gu칤a puede ser:
     
     ```bash
       host = postgres-db
       port = 5432
       Database Name = landing_zone
       username = sib_user
       password = sib_user
     ```
   - Click en `Set up source`.

2. **Crea un destino**:

   - Ve al apartado de Destinos y click en `+ New destination`.
   - Busca el conector par `Clickhouse`.
    - Ajusta los campos seg칰n tus variables de entorno. Una gu칤a puede ser:
      
     ```bash
       host = localhost
       port = 8123
       DB Name = stage
       user = default
     ```
   - Click en `Set up destination`.

3. **Crear una conexi칩n**:

   - Ve al apartado de Conexiones y click en `+ New connection`.
   - Selecciona la fuente y el destino que acabas de crear.
   - Coloca los detalles de las conexiones como sea necesario.
   - Click en `Set up connection`.

Est치s listo! Tu conexi칩n est치 configurada y lista para usarse!游꿀 

4. **Copiar Connection-id para poder ejecutar el sync desde Airflow**:

   - En la URL de la conexi칩n copia lo que est치 luego del path `/connections/`
   - Pega ese ID en la variable de entorno `AIRBYTE_CONNECTION_ID`
   - Reinicia el proyecto principal

    ```bash
    cd sib/
    docker-compose up down
    docker-compose up -d
   ```

## 5. Configurar conector de Airbyte desde la UI en Airflow

Inicia en la UI de Airflow accediendo a http://localhost:8080/ en tu navegador. El usuario y contrase침a por defecto es airflow. Luego:

   - Ve al apartado de Conexiones y click en `+ New connection`.
   - Elige el tipo de conexi칩n de Airbyte
   - Ajusta los campos con la siguiente guia:
     
     ```bash
       Connection Id = airbyte_conn_id
       host = airbyte-proxy
       login = airbyte
       password = password
       port = 8001
     ```
     
   - Click en `Save`.
## 6. Orquestaci칩n con Airflow

Ahora que todo est치 configurado, es tiempo de correr el pipeline!

- En la UI de Airflow, ve al apartado de "DAGs"
- Localiza `extract_info_banks_stocks` y click en "Trigger DAG".

Esto iniciar치 el pipeline completo, comenzando con la extracci칩n de la data desde yfinance, almacenando esa data en PostgreSQL, luego ejecutando el proceso de `sync` de Airbyte para transportar la data cruda desde Postgres a Clickhouse. Y por 칰ltimo, corriendo las validaciones y transformaciones con DBT para tener como resultado la data para ser utilizada por los analistas.

- Confirma el estado de la sincronizaci칩n en Airbyte.
- Luego de que el job `send_to_datawarehouse` se ejecute, puedes observar la data en clickhouse en el esquema definido en la conexi칩n del destino. Si no definiste esquema estar치 en el esquema `_airbyte_internal_`.
- Una vez que el proceso anterior se complete, se ejecutar치 un trigger que ejecutar치 el DAG `execute_dbt_jobs`. Este es el encargado de ejecutar las validaciones y transformaciones de los datos en stage.
- Cuando el proceso anterior culmine, puedes observar en la base de datos de clickhouse en el esquema `dwh` estar치n los modelos y vistas creadas con DBT.


# Demo visual en Youtube

[![Demo Pipeline](https://img.youtube.com/vi/aOJAdJFkF28/0.jpg)](https://www.youtube.com/watch?v=aOJAdJFkF28)






